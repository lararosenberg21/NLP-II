{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c294fe57-c23f-4e78-af6d-5612a189a6fc",
   "metadata": {},
   "source": [
    "## TRABAJO PRACTICO 3: CHAT PARA CONSULTAR DISTINTOS CVs A TRAVES DE UN AGENTE\n",
    "ALUMNA: Lara Rosenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5639934e-1def-4601-8a3c-49d763416c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\damia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\damia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importamos las librerias necesarias\n",
    "import os\n",
    "import streamlit as st\n",
    "from pinecone import Pinecone\n",
    "from groq import Groq\n",
    "from typing import List, Set\n",
    "import nltk\n",
    "import re\n",
    "import unicodedata\n",
    "nltk.download(\"punkt_tab\")\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0d53de-626b-407b-aafd-fe38257decd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos las API KEY necesarias (Pinecone y Groq)\n",
    "os.environ['PINECONE_API_KEY'] = ''\n",
    "os.environ['GROQ_API_KEY'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8308d12-75f1-46db-8cd8-dbb3a33992e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcion para particionar el texto\n",
    "def read_and_chunk_sentences(\n",
    "    file_path: str,\n",
    "    chunk_size: int = 40,\n",
    "    overlap: int = 10\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Lee el archivo de texto, lo separa en oraciones y hace el chunk con overlap.\n",
    "\n",
    "    Argumentos:\n",
    "        file_path (str): Path al archivo de texto.\n",
    "        chunk_size (int): Numero de oraciones por chunk.\n",
    "        overlap (int): Numero de oraciones que estan en overlap.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: Lista de chunks.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"{file_path} does not exist.\")\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    sentences = nltk.sent_tokenize(text, language='spanish')\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    while i < len(sentences):\n",
    "        chunk = sentences[i:i+chunk_size]\n",
    "        if chunk:\n",
    "            chunks.append(\" \".join(chunk))\n",
    "        i += chunk_size - overlap\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ddaac53-8b31-4a5d-a864-bf5bd66a5216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciamos las API KEY necesarias\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f41979-82c2-46fd-9b7f-f581d4a54944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializamos cliente Pinecone\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "# Función para subir los documentos a Pinecone\n",
    "def upload_documents(file_path: str, index_name: str, namespace: str):\n",
    "    # Leemos el archivo y lo dividimos en chunks\n",
    "    chunks = read_and_chunk_sentences(file_path, chunk_size=5, overlap=2)\n",
    "\n",
    "    # Creamos los documentos en el formato requerido por Pinecone\n",
    "    documents = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        documents.append({\n",
    "            \"_id\": f\"{namespace}_chunk_{i + 1}\",\n",
    "            \"chunk_text\": chunk,\n",
    "            \"category\": namespace  # Guardamos la categoría como el namespace\n",
    "        })\n",
    "    print(documents)\n",
    "\n",
    "    # Conectamos al índice en Pinecone\n",
    "    index_pc = pc.Index(index_name)\n",
    "\n",
    "    # Subimos los documentos al índice\n",
    "    index_pc.upsert_records(namespace=namespace, records=documents)\n",
    "    print(f\"Subidos {len(documents)} chunks al índice '{index_name}' en el namespace '{namespace}'.\")\n",
    "\n",
    "# Definimos nombres de los archivos y sus namespaces\n",
    "files_and_namespaces = [\n",
    "    (\"CV LARA ROSENBERG.txt\", \"cv-lara-ns\"),\n",
    "    (\"CV VICTORIA TERAN.txt\", \"cv-victoria-ns\"),\n",
    "    (\"CV CLAUDIO BARRIL.txt\", \"cv-claudio-ns\")\n",
    "]\n",
    "\n",
    "# Creamos un índice por cada CV y subimos los documentos a su respectivo índice\n",
    "for file, namespace in files_and_namespaces:\n",
    "    index_name = f\"{namespace}-index\"  # Usamos el namespace para crear un índice único para cada CV\n",
    "\n",
    "    # Verificamos si el índice ya existe. Si no, lo creamos.\n",
    "    if not pc.list_indexes() or index_name not in [i.name for i in pc.list_indexes()]:\n",
    "        pc.create_index_for_model(\n",
    "            name=index_name,\n",
    "            cloud=\"aws\",\n",
    "            region=\"us-east-1\",\n",
    "            embed={\n",
    "                \"model\": \"llama-text-embed-v2\",\n",
    "                \"field_map\": {\"text\": \"chunk_text\"}\n",
    "            }\n",
    "        )\n",
    "        print(f\"Índice '{index_name}' creado.\")\n",
    "\n",
    "    # Subimos los documentos al índice específico\n",
    "    #upload_documents(file, index_name, namespace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37fc030-8e56-43ae-bc95-893be3a70680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para normalizar texto (quitar tildes, pasar a minúsculas)\n",
    "def normalizar(texto: str) -> str:\n",
    "    texto = texto.lower()\n",
    "    texto = unicodedata.normalize('NFD', texto)\n",
    "    return ''.join(c for c in texto if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "# Patrones para identificar las personas mencionadas en la consulta\n",
    "person_patterns = {\n",
    "    \"cv-lara-ns-index\": re.compile(r\"\\blara(\\s+rosenberg)?\\b|\\brosenberg\\b\", re.IGNORECASE),\n",
    "    \"cv-victoria-ns-index\": re.compile(r\"\\bvictoria(\\s+teran)?\\b|\\bteran\\b\", re.IGNORECASE),\n",
    "    \"cv-claudio-ns-index\": re.compile(r\"\\bclaudio(\\s+barril)?\\b|\\bbarril\\b\", re.IGNORECASE)\n",
    "}\n",
    "\n",
    "# Función para identificar las personas mencionadas --> devuelve un diccionario con los nombres de los indices de las personas que identifico\n",
    "def identificar_personas_mencionadas(texto: str) -> Set[str]:\n",
    "    texto_normalizado = normalizar(texto)\n",
    "    indices_decididos = set()\n",
    "\n",
    "    for idx, pat in person_patterns.items():\n",
    "        if pat.search(texto_normalizado):\n",
    "            indices_decididos.add(idx)\n",
    "\n",
    "    return indices_decididos\n",
    "\n",
    "# Función para buscar los chunks más relevantes en Pinecone\n",
    "def search_similar(texto, top_k=10, indices=None):\n",
    "    contexto = \"\"\n",
    "\n",
    "    if not indices:\n",
    "        indices = [\"cv-lara-ns-index\"]\n",
    "\n",
    "    # Realizamos la búsqueda.\n",
    "    for indice in indices:\n",
    "        namespace = indice.replace(\"-index\", \"\")\n",
    "        results = pc.Index(indice).search(\n",
    "            namespace=namespace,\n",
    "            query={\n",
    "                \"top_k\": top_k,\n",
    "                \"inputs\": {\n",
    "                    'text': texto\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "        # Recopilamos los resultados y los agregamos al contexto\n",
    "        contexto += \"\\n\".join([hit['fields']['chunk_text'] for hit in results['result']['hits']])\n",
    "\n",
    "    return contexto\n",
    "\n",
    "# Definirmos el agente para generar la respuesta usando Groq\n",
    "class Agent:\n",
    "    def __init__(self, client, model=\"meta-llama/llama-4-scout-17b-16e-instruct\"):\n",
    "        self.client = client\n",
    "        self.model = model\n",
    "        self.messages = []\n",
    "\n",
    "    def add_message(self, role, content):\n",
    "        self.messages.append({\"role\": role, \"content\": content})\n",
    "\n",
    "    def __call__(self, message):\n",
    "        self.add_message(\"user\", message)\n",
    "        result = self.execute(message)\n",
    "        self.add_message(\"assistant\", result)\n",
    "        return result\n",
    "\n",
    "    def execute(self, message):\n",
    "        # Detectamos la persona mencionada en la consulta\n",
    "        indices_decididos = identificar_personas_mencionadas(message)\n",
    "\n",
    "        # Si no se mencionan personas, utilizamos por default el cv de Lara\n",
    "        if not indices_decididos:\n",
    "            indices_decididos = [\"cv-lara-ns-index\"]\n",
    "\n",
    "        # Buscamos el contexto relevante\n",
    "        contexto = search_similar(message, indices=indices_decididos)\n",
    "\n",
    "        # Generamos la respuesta con el contexto obtenido\n",
    "        prompt = f\"\"\"\n",
    "Sos un asistente conversacional. Respondé de forma natural y conversacional.\n",
    "\n",
    "Responde basandote en el contexto. Si no tenes informacion, aclaralo.\n",
    "Consulta previa (historial): {self.messages}\n",
    "\n",
    "Consulta: {message}\n",
    "\n",
    "Contexto:\n",
    "{contexto}\n",
    "\"\"\"\n",
    "        self.add_message(\"user\", prompt.strip())\n",
    "\n",
    "        completion = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=self.messages,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        return completion.choices[0].message.content\n",
    "\n",
    "# Inicializamos el agente con Groq\n",
    "groq_client = Groq(api_key=GROQ_API_KEY)\n",
    "agente = Agent(client=groq_client)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba7e7295-717c-46d3-8607-d3b2ebcfd96b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\damia\\Anaconda3\\envs\\nuevo_entorno\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Según la información que tengo, Victoria trabaja o ha trabajado en al menos 3 empresas:\n",
      "\n",
      "1. Mercado Libre (2022 - actualidad) como Analista Sr de Riesgo de Crédito\n",
      "2. AFIP - Administración Federal de Ingresos Públicos (2019 - 2022) como Analista de Datos\n",
      "3. HSBC (2015) como pasante\n",
      "\n",
      "También tuvo un rol como Analista Administrativo Contable desde 2015 hasta 2019, pero no especifica si fue en una empresa en particular o si fue en alguna de las mencionadas anteriormente.\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo de consulta\n",
    "consulta_usuario = \"En cuantas empresas trabajo Victoria?\"\n",
    "respuesta = agente(consulta_usuario)\n",
    "print(respuesta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08a0ff12-6c42-43a7-af05-a5da711b41f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Según la información que tengo, Lara trabaja o ha trabajado en al menos 6 empresas:\n",
      "\n",
      "1. Mercado Libre (Marzo 2023 - actualidad) en dos roles diferentes: \n",
      "   - Credit Risk Modeling Supervisor (Marzo 2024 - actualidad)\n",
      "   - Credit Risk Modeling Sr Analyst (Marzo 2023 - Marzo 2024)\n",
      "\n",
      "2. Algorithia (Noviembre 2021 - Marzo 2023) como Data Scientist\n",
      "\n",
      "3. Círculo de Crédito (Marzo 2020 - Noviembre 2021) como Consultor de Modelos\n",
      "\n",
      "4. Banco Patagonia (Septiembre 2019 - Marzo 2020) como Analista de Riesgos Financieros\n",
      "\n",
      "5. Instituto Argentino de Mercado de Capitales (Agosto 2018 - Agosto 2019) como Analista Financiero\n",
      "\n",
      "6. MetLife Seguros (Abril 2018 - Agosto 2018) como pasante\n",
      "\n",
      "Espero que esta información sea útil. ¿Necesitas algo más?\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo de consulta\n",
    "consulta_usuario = \"En cuantas empresas trabajo Lara?\"\n",
    "respuesta = agente(consulta_usuario)\n",
    "print(respuesta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17db2787-a451-434c-b6ac-068ad18b6afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sí, hay una empresa en común entre Lara, Victoria y Claudio.\n",
      "\n",
      "**Mercado Libre** es la empresa que tienen en común.\n",
      "\n",
      "* Lara trabaja en Mercado Libre desde Marzo 2023 como Credit Risk Modeling Supervisor.\n",
      "* Victoria trabaja en Mercado Libre desde 2022 como Analista Sr de Riesgo de Crédito.\n",
      "* Claudio trabajó en Mercado Libre desde Marzo 2019 hasta Diciembre 2021 como Líder Técnico de Desarrollo de Software y luego como Ingeniero de Software Senior.\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo de consulta\n",
    "consulta_usuario = \"Hay alguna empresa en comun entre Lara, Victoria y Claudio?\"\n",
    "respuesta = agente(consulta_usuario)\n",
    "print(respuesta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02877f8-34db-4783-9120-b97f0885c07a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nuevo_entorno)",
   "language": "python",
   "name": "nuevo_entorno"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
